{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Stackoverflow Post Tag Prediction<h1/><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we'll turn it to a binary classification problem. We'll fetch the rows for the tags - 'iphone' and 'android' only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"stack-overflow-data.csv\")\n",
    "data = data[data['tags'].isin(['iphone', 'android'])].reset_index().drop('index', axis=1)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>identifying server timeout quickly in iphone  ...</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distance between 2 or more drop pins  i was do...</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what are all the restrictions by apple for iph...</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not able to clicked on item  i have facing ver...</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all phone numbers of one contact for startacti...</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post     tags\n",
       "0  identifying server timeout quickly in iphone  ...   iphone\n",
       "1  distance between 2 or more drop pins  i was do...   iphone\n",
       "2  what are all the restrictions by apple for iph...   iphone\n",
       "3  not able to clicked on item  i have facing ver...   iphone\n",
       "4  all phone numbers of one contact for startacti...  android"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android    2000\n",
       "iphone     2000\n",
       "Name: tags, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tags.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Preprocessing (Noise Removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "data['post'] = data['post'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [identifying, server, timeout, quickly, in, ip...\n",
       "1    [distance, between, 2, or, more, drop, pins, i...\n",
       "2    [what, are, all, the, restrictions, by, apple,...\n",
       "3    [not, able, to, clicked, on, item, i, have, fa...\n",
       "4    [all, phone, numbers, of, one, contact, for, s...\n",
       "Name: post, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "tokens = data['post'].apply(word_tokenize)\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install contractions\n",
    "# !pip install inflect\n",
    "# !pip install autocorrect\n",
    "# import contractions\n",
    "import inflect\n",
    "from autocorrect import spell\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "# def replace_contractions(words):\n",
    "#     \"\"\"Replace contractions in string of text\"\"\"\n",
    "#     return contractions.fix(words)\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def spelling_correction(words):\n",
    "    \"\"\"Autocorrect the spelling of the words\"\"\"\n",
    "    new_words = [spell(w) for w in (words)]\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens.apply(remove_non_ascii)\n",
    "tokens = tokens.apply(to_lowercase)\n",
    "tokens = tokens.apply(remove_punctuation)\n",
    "tokens = tokens.apply(replace_numbers)\n",
    "tokens = tokens.apply(remove_stopwords)\n",
    "# tokens = tokens.apply(spelling_correction) # Takes time\n",
    "tokens = tokens.apply(stem_words)\n",
    "tokens = tokens.apply(lemmatize_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_sentence(words):\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "X = tokens\n",
    "y = data['tags'].replace({'iphone':1, 'android':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Feature extraction using word2vec Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv_model = KeyedVectors.load_word2vec_format('pretrained models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: Mean of Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embedding_vectorizer(wv_model, docs):\n",
    "    mean_vecs = []\n",
    "    for words in docs:\n",
    "        vecs = []\n",
    "        for word in words:\n",
    "            if word in wv_model.vocab:\n",
    "                vecs.append(wv_model[word])\n",
    "            else:\n",
    "                vecs.append(np.zeros(wv_model.vector_size))\n",
    "        \n",
    "        mean_vecs.append(np.mean(vecs, axis=0))\n",
    "        \n",
    "    return mean_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mev = mean_embedding_vectorizer(wv_model, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: Mean of Word2Vec vectors with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "def tfidf_embedding_vectorizer(wv_model, docs):\n",
    "    tfidf = TfidfVectorizer().fit(docs.apply(lambda words:' '.join(words)))\n",
    "    word2weight = defaultdict(lambda: max(tfidf.idf_), \n",
    "                              [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "    tfidf_vecs = []\n",
    "\n",
    "    for words in docs:\n",
    "        vecs = []\n",
    "        for word in words:\n",
    "            if word in wv_model.vocab:\n",
    "                vecs.append(wv_model[word] * word2weight[word])\n",
    "            else:\n",
    "                vecs.append(np.zeros(wv_model.vector_size))\n",
    "\n",
    "        tfidf_vecs.append(np.mean(vecs, axis=0))\n",
    "    \n",
    "    return tfidf_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = tfidf_embedding_vectorizer(wv_model, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(mev).add_prefix('feat_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_290</th>\n",
       "      <th>feat_291</th>\n",
       "      <th>feat_292</th>\n",
       "      <th>feat_293</th>\n",
       "      <th>feat_294</th>\n",
       "      <th>feat_295</th>\n",
       "      <th>feat_296</th>\n",
       "      <th>feat_297</th>\n",
       "      <th>feat_298</th>\n",
       "      <th>feat_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.054047</td>\n",
       "      <td>-0.006847</td>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.092404</td>\n",
       "      <td>-0.068242</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>-0.007483</td>\n",
       "      <td>-0.065513</td>\n",
       "      <td>-0.023434</td>\n",
       "      <td>-0.040749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058644</td>\n",
       "      <td>0.006197</td>\n",
       "      <td>-0.115312</td>\n",
       "      <td>0.026849</td>\n",
       "      <td>0.019679</td>\n",
       "      <td>-0.105075</td>\n",
       "      <td>0.007112</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>-0.080108</td>\n",
       "      <td>-0.044161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.013730</td>\n",
       "      <td>-0.021120</td>\n",
       "      <td>0.025297</td>\n",
       "      <td>0.098415</td>\n",
       "      <td>-0.046692</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>-0.036435</td>\n",
       "      <td>-0.089058</td>\n",
       "      <td>0.039370</td>\n",
       "      <td>0.037308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.089198</td>\n",
       "      <td>-0.060200</td>\n",
       "      <td>0.023799</td>\n",
       "      <td>-0.017523</td>\n",
       "      <td>-0.080523</td>\n",
       "      <td>-0.075071</td>\n",
       "      <td>-0.028831</td>\n",
       "      <td>-0.032314</td>\n",
       "      <td>0.080765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.125996</td>\n",
       "      <td>0.047470</td>\n",
       "      <td>-0.037542</td>\n",
       "      <td>0.110095</td>\n",
       "      <td>-0.086817</td>\n",
       "      <td>-0.004518</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>-0.023903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055047</td>\n",
       "      <td>0.065123</td>\n",
       "      <td>-0.056684</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>-0.085679</td>\n",
       "      <td>-0.055781</td>\n",
       "      <td>0.018345</td>\n",
       "      <td>-0.061113</td>\n",
       "      <td>-0.049070</td>\n",
       "      <td>0.052026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.022023</td>\n",
       "      <td>0.044843</td>\n",
       "      <td>-0.044099</td>\n",
       "      <td>0.100669</td>\n",
       "      <td>-0.100849</td>\n",
       "      <td>0.053145</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>-0.030137</td>\n",
       "      <td>0.029960</td>\n",
       "      <td>-0.021439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074973</td>\n",
       "      <td>0.117831</td>\n",
       "      <td>-0.011440</td>\n",
       "      <td>0.046054</td>\n",
       "      <td>0.051869</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.003928</td>\n",
       "      <td>-0.084062</td>\n",
       "      <td>-0.052671</td>\n",
       "      <td>0.045854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046797</td>\n",
       "      <td>-0.029014</td>\n",
       "      <td>-0.044262</td>\n",
       "      <td>0.047656</td>\n",
       "      <td>-0.110386</td>\n",
       "      <td>0.056220</td>\n",
       "      <td>0.062416</td>\n",
       "      <td>-0.082308</td>\n",
       "      <td>0.077633</td>\n",
       "      <td>0.054741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028023</td>\n",
       "      <td>0.072235</td>\n",
       "      <td>-0.063314</td>\n",
       "      <td>0.008243</td>\n",
       "      <td>-0.023574</td>\n",
       "      <td>-0.004674</td>\n",
       "      <td>-0.012499</td>\n",
       "      <td>-0.085481</td>\n",
       "      <td>-0.019483</td>\n",
       "      <td>-0.014777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_0    feat_1    feat_2    feat_3    feat_4    feat_5    feat_6  \\\n",
       "0 -0.054047 -0.006847  0.003345  0.092404 -0.068242  0.055398 -0.007483   \n",
       "1 -0.013730 -0.021120  0.025297  0.098415 -0.046692 -0.002546 -0.036435   \n",
       "2 -0.125996  0.047470 -0.037542  0.110095 -0.086817 -0.004518  0.030500   \n",
       "3 -0.022023  0.044843 -0.044099  0.100669 -0.100849  0.053145  0.070942   \n",
       "4  0.046797 -0.029014 -0.044262  0.047656 -0.110386  0.056220  0.062416   \n",
       "\n",
       "     feat_7    feat_8    feat_9    ...     feat_290  feat_291  feat_292  \\\n",
       "0 -0.065513 -0.023434 -0.040749    ...     0.058644  0.006197 -0.115312   \n",
       "1 -0.089058  0.039370  0.037308    ...     0.019272  0.089198 -0.060200   \n",
       "2  0.004625 -0.025803 -0.023903    ...     0.055047  0.065123 -0.056684   \n",
       "3 -0.030137  0.029960 -0.021439    ...     0.074973  0.117831 -0.011440   \n",
       "4 -0.082308  0.077633  0.054741    ...     0.028023  0.072235 -0.063314   \n",
       "\n",
       "   feat_293  feat_294  feat_295  feat_296  feat_297  feat_298  feat_299  \n",
       "0  0.026849  0.019679 -0.105075  0.007112  0.001790 -0.080108 -0.044161  \n",
       "1  0.023799 -0.017523 -0.080523 -0.075071 -0.028831 -0.032314  0.080765  \n",
       "2  0.031982 -0.085679 -0.055781  0.018345 -0.061113 -0.049070  0.052026  \n",
       "3  0.046054  0.051869 -0.052432 -0.003928 -0.084062 -0.052671  0.045854  \n",
       "4  0.008243 -0.023574 -0.004674 -0.012499 -0.085481 -0.019483 -0.014777  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84875"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1234).fit(X_train, y_train.values.ravel())\n",
    "model.score(X_test, y_test.values.ravel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
